{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengumpulan Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup AUTH Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagian Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tambahkan ' -filter:retweets' untuk menghindari RETWEET pada tweet yang ingin di crawling\n",
    "kataKunci = 'belajar di rumah -filter:retweets'\n",
    "\n",
    "# ===================================================================== \n",
    "# \n",
    "# [START] Crawling berdasarkan rentang waktu (HANYA DAPAT MENGAMBIL TWEET DALAM 7 HARI TERAKHIR)\n",
    "# Sumber : https://gist.github.com/alexdeloy/fdb36ad251f70855d5d6\n",
    "#\n",
    "# ===================================================================== \n",
    "\n",
    "# mengambil tweet pertama antara rentang waktu (tanggal awal & akhir)\n",
    "startDate = datetime.datetime(2021, 5, 6, 0, 0, 0) # tanggal awal = '6 Mei 2021 jam 00:00:00'\n",
    "endDate = datetime.datetime(2021, 5, 10, 0, 0, 0) # tanggala akhir = '10 Mei 2021 jam 00:00:00'\n",
    "\n",
    "tweets = [] # array/wadah kosong\n",
    "\n",
    "tmpTweets = api.search(kataKunci, lang='id', tweet_mode='extended', count='100')\n",
    "for tweet in tmpTweets:\n",
    "    if tweet.created_at < endDate and tweet.created_at > startDate:\n",
    "        tweets.append(tweet)\n",
    "\n",
    "while (tmpTweets[-1].created_at > startDate):\n",
    "    print(\"Last Tweet @\", tmpTweets[-1].created_at, \" - fetching some more\")\n",
    "    tmpTweets = api.search(kataKunci, lang='id', tweet_mode='extended', count='100', max_id = tmpTweets[-1].id)\n",
    "    for tweet in tmpTweets:\n",
    "        if tweet.created_at < endDate and tweet.created_at > startDate:\n",
    "            tweets.append(tweet)\n",
    "\n",
    "print('\\nSELESAI CRAWLING')\n",
    "# ===================================================================== \n",
    "# \n",
    "# [END] Crawling berdasarkan rentang waktu (HANYA DAPAT MENGAMBIL TWEET DALAM 7 HARI TERAKHIR)\n",
    "# Sumber : https://gist.github.com/alexdeloy/fdb36ad251f70855d5d6\n",
    "#\n",
    "# ===================================================================== \n",
    "\n",
    "\n",
    "# Wadah kosong\n",
    "id = []\n",
    "text = []\n",
    "created_at = []\n",
    "username = []\n",
    "\n",
    "for data in tweets:\n",
    "    # mengisi wadah kosong menggunakan data dari API)\n",
    "    id.append( data._json['id'] )\n",
    "    text.append( data._json['full_text'] )\n",
    "    created_at.append( data._json['created_at'] )\n",
    "    username.append( data._json['user']['screen_name'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Array menjadi Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# Mengubah kumpulan array menjadi sebuah dataFrame\n",
    "dataFrame = pandas.DataFrame({\n",
    "    'id': id,\n",
    "    'tweet': text,\n",
    "    'dibuat pada': created_at,\n",
    "    'username': username\n",
    "})\n",
    "\n",
    "dataFrame\n",
    "\n",
    "# export dataFrame ke excel (.xlsx)\n",
    "# dataFrame.to_excel('crawling_09-05-2021.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
